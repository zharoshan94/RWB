{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a92ea7-ac5b-48b6-ac2b-7baf55650b25",
   "metadata": {},
   "source": [
    "# Detection of Rossby wave breaking by using two different indices\n",
    "\n",
    "Author: Severin Kaderli - University of Bern <br> \n",
    "Mail: severin.kaderli@students.unibe.ch\n",
    "\n",
    "This algorithm detects and classifies Rossby Wave Breaking (RWB). <br>\n",
    "An xarray data set including the dimensions lat, lot at a specific time step is used as an input. <br>\n",
    "\n",
    "The algorithms follows these steps:\n",
    "* Step 1: Extract a contour line of a specific value (see Contour Extraction Algorithm script also avaiable on GitHub) \n",
    "* Step 2: Apply one of two indices:\n",
    "    - Streamer index by Wernli and Sprenger (2007). <br> The events are distinguished by stratospheric/tropospheric streamers or by cyclonic/anticyclonic streamers. <br> For the distinction between cyclonic/anticyclonic events, the momentum flux must be provided. \n",
    "    - Overturning index by Barnes and Hartmann (2012). <br> The events are distinguished by cyclonic/anticyclonic by using two different methods. \n",
    "* Step 3: Save results\n",
    "* Step 4: Visualize results. \n",
    "\n",
    "## Preparations\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa13ba4-1a2f-4d20-ab86-9ae7cfb8b9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for data\n",
    "import xarray as xr\n",
    "import metpy.calc as mpcalc\n",
    "\n",
    "# for algorithm\n",
    "import numpy as np\n",
    "import itertools as itertools\n",
    "from skimage import measure\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString\n",
    "from matplotlib.path import Path\n",
    "from scipy import ndimage\n",
    "from math import sqrt\n",
    "from skimage.draw import line\n",
    "from tqdm import tqdm\n",
    "import wrf as wrf\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ce73b-97fb-4638-975b-68b7ac8f56f9",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc1ffcb-a0bf-436b-8631-093a482596a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify names of the coordinates\n",
    "lat_name = \"lat\"\n",
    "lon_name = \"lon\"\n",
    "time_name = \"time\"\n",
    "\n",
    "# load data\n",
    "ds_var = xr.open_dataset(\"/scratch3/severin/data/era5/pv/pv330_1979-2019.nc\")\n",
    "ds_flux =xr.open_dataset(\"/scratch3/severin/data/era5/wind/flux_zm_1979-2019.nc\")\n",
    "\n",
    "# combine in one dataset\n",
    "ds = ds_var\n",
    "ds[\"flux\"] = ds_flux.flux\n",
    "\n",
    "# Select time steps, variable, contour level, and index\n",
    "ds = ds.isel(time=slice(0,365))\n",
    "var_name = \"pv\"\n",
    "level = 2\n",
    "index = \"streamer\" #\"streamer\" or \"ot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c138f4a-ede8-4ac1-8241-92a79b818ba7",
   "metadata": {},
   "source": [
    "## Subroutines\n",
    "### Contour Extraction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1382282-86ef-438d-bad2-fc787ba340fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_contour(dataset, level, y_overlap = 1, x_extent = 0.3, single = True, scale = True):\n",
    "    \"\"\"\n",
    "    Extracts contour by using the \"find_contours\" function of \"measure\" by \"skimage\".\n",
    "    The coordinates of the contour points can be mapped on a grid with the same resolution as the input field.\n",
    "    Unclosed contours at longitude border according to their start and end point can be grouped. \n",
    "    Contours with a too small extent in x direction can be dropped.\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    dataset: xarray\n",
    "        xarray without time coordinate\n",
    "        \n",
    "    level: float\n",
    "        contour level\n",
    "        \n",
    "    y_overlap: float, optional\n",
    "        Overlap in y direction in degrees in between unclosed contours at longitude border are grouped\n",
    "    \n",
    "    x_extent: float, optional\n",
    "        Set minimal extents of a contour in the x direction. A coverage of all longitudes means x_extent = 1.\n",
    "        \n",
    "    single: boolean, optional\n",
    "        Select if only one contour (largest with contour with smallest mean latitude) should be kept\n",
    "    \n",
    "    scale: boolean, optional\n",
    "        Select if the contour should be rescaled on a grid with the same resolution as the input dataset\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    dataframe: float\n",
    "        Dataframe with columns lon and lat\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #**************************\n",
    "    #define subroutines\n",
    "    \n",
    "    def get_contour(dataset, level):\n",
    "        \"\"\"\n",
    "        Extracts contour by using the \"find_contours\" function of \"measure\" by \"skimage\".\n",
    "        Output is modified in order to fit the original coordinates.\n",
    "        \n",
    "        \"\"\"\n",
    "        return [np.c_[contour[:,1]+min(ds[lon_name].values),contour[:,0]] for contour in measure.find_contours(dataset[var_name].values,level)]\n",
    "    \n",
    "    def rescale_contours(contours):\n",
    "        \"\"\"\n",
    "        Rescale the coordinates of the contour points on a grid with the same resolution as the input field.\n",
    "        \n",
    "        \"\"\"\n",
    "        x, y = np.meshgrid(ds[lon_name],ds[lat_name]) \n",
    "        x, y = x.flatten(), y.flatten()\n",
    "        grid_points = np.vstack((x,y)).T \n",
    "        tree = spatial.KDTree(grid_points)\n",
    "\n",
    "        contours_scaled = []\n",
    "        for contour in contours:\n",
    "            temp = []\n",
    "            for point in contour:\n",
    "                temp.append(grid_points[tree.query([point])[1][0]])\n",
    "            unique = list(dict.fromkeys(map(tuple,temp)))\n",
    "            contours_scaled.append(np.asarray(unique))\n",
    "\n",
    "        return contours_scaled\n",
    "    \n",
    "    def group_contours(contours, y_overlap):\n",
    "        \"\"\"\n",
    "        Group unclosed contours at longitude border according to their start and end point.\n",
    "        \n",
    "        \"\"\"\n",
    "        contours_index = list(range(0,len(contours)))\n",
    "        borders = []\n",
    "        for contour,index in zip(contours,contours_index):\n",
    "            stp = contour[0].tolist()\n",
    "            ndp = contour[-1].tolist()\n",
    "            borders.append([index]+stp+ndp)\n",
    "\n",
    "        start = [[border[1],border[2]] for border in borders]\n",
    "        end = [[border[3],border[4]] for border in borders]\n",
    "        both = start+end\n",
    "\n",
    "        lon_border = [int(ds[lon_name].values.min()), int(ds[lon_name].values.max())]\n",
    "\n",
    "        for point in both:\n",
    "            ind = [i for i, x in enumerate(both) if (x[0]==point[0] or all(i in [x[0],point[0]] for i in lon_border)) and point[1]-y_overlap <= x[1] <= point[1]+y_overlap]\n",
    "            ind = np.mod(ind,len(contours))\n",
    "            add = [borders[i][0] for i in ind]\n",
    "            for i in ind:\n",
    "                borders[i][0] = borders[min(add)][0]    \n",
    "\n",
    "        all_values = [border[0] for border in borders]\n",
    "        unique_values = set(all_values)\n",
    "\n",
    "        contours_grouped = []\n",
    "        for value in unique_values:\n",
    "            this_group = []\n",
    "            for border,contour in zip(borders,contours):\n",
    "                if border[0] == value:\n",
    "                    this_group.append(contour)\n",
    "            contours_grouped.append(this_group)\n",
    "\n",
    "        contours_grouped_sorted = []\n",
    "        for group in contours_grouped:\n",
    "            if len(group) > 1:\n",
    "                bigest = sorted(group, key=len, reverse=True)[0]\n",
    "                rest = sorted(group, key=len, reverse=True)[1:]\n",
    "\n",
    "                temp = [bigest]\n",
    "                while len(rest) > 0:\n",
    "                    test = temp[-1][-1,1]\n",
    "                    for item,ind in zip(rest, range(0,len(rest))):\n",
    "                        if test-y_overlap <= item[0,1] <= test+y_overlap:\n",
    "                            temp.append(item)\n",
    "                            break\n",
    "                    del rest[ind]\n",
    "                contours_grouped_sorted.append(np.asarray(list(itertools.chain.from_iterable(temp))))\n",
    "            else:\n",
    "                contours_grouped_sorted.append(np.asarray(list(itertools.chain.from_iterable(group))))\n",
    "\n",
    "        return contours_grouped_sorted\n",
    "    \n",
    "    def filter_contours(contours, x_extent):\n",
    "        \"\"\"\n",
    "        Contours with a too small extent in x direction are dropped.\n",
    "\n",
    "        \"\"\"\n",
    "        lons = ds[lon_name].values\n",
    "        contour_extent = [len(np.unique(np.round(contour[:,0]))) for contour in contours]\n",
    "        test_extent = [extent/len(lons) >= x_extent for extent in contour_extent]\n",
    "\n",
    "        return list(itertools.compress(contours, test_extent))\n",
    "    \n",
    "    def single_contours(contours):\n",
    "        \"\"\"\n",
    "        Keeps largest contour.\n",
    "        If there are two contours fully encircling the pole, the one with the smaller mean latitude is kept. \n",
    "\n",
    "        \"\"\"\n",
    "        lons = ds[lon_name].values\n",
    "        contour_extent = [len(np.unique(np.round(contour[:,0]))) for contour in contours]\n",
    "\n",
    "        if sum([i==1 for i in contour_extent])>1: \n",
    "            mean_lat = [np.mean(contour[:,1]) for contour in contours]\n",
    "            contours_single = contours[mean_lat.index(min(mean_lat))]\n",
    "        else: \n",
    "            contours_single = contours[contour_extent.index(max(contour_extent))]\n",
    "\n",
    "        return contours_single\n",
    "\n",
    "    def df_contours(contours):\n",
    "        \"\"\"\n",
    "        Store final contours in a pandas dataframe\n",
    "\n",
    "        \"\"\"\n",
    "        if type(contours)==list:\n",
    "            temp = np.asarray(list(itertools.chain.from_iterable(contours)))\n",
    "        else:\n",
    "            temp = contours\n",
    "        return pd.DataFrame({'lon': temp[:,0].tolist(), 'lat': temp[:,1].tolist()})\n",
    "    \n",
    "    #**************************\n",
    "\n",
    "    contours = get_contour(dataset, level)\n",
    "    \n",
    "    if scale == True:\n",
    "        contours_scaled = rescale_contours(contours)\n",
    "    else:\n",
    "        contours_scaled = contours\n",
    "        \n",
    "    contours_grouped = group_contours(contours_scaled, y_overlap)\n",
    "    \n",
    "    contours_filtered = filter_contours(contours_grouped, x_extent)\n",
    "    \n",
    "    if contours_filtered !=[]:\n",
    "        if single == True:\n",
    "            contours_return = single_contours(contours_filtered)\n",
    "        else:\n",
    "            contours_return = contours_filtered\n",
    "            \n",
    "        return df_contours(contours_return)\n",
    "            \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b93d20-bf16-4e48-95d2-a5a15b9d25b4",
   "metadata": {},
   "source": [
    "### Utility and Index functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec98e832-c4c0-4bad-921e-f581690e2946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the Earth\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    lon1, lat1, lon2, lat2: float\n",
    "        Longitude and latitude values of both points\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    km: float\n",
    "        Great circle distance in km\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "def map_on_grid(obj,value=1):\n",
    "    \"\"\"\n",
    "    Map a objetct with coordinate points on a grid with the same resolution as the input field\n",
    "    \n",
    "    Input: \n",
    "    -----\n",
    "    obj: dataframe\n",
    "        Object in form of a dataframe with the colmuns lon and lat\n",
    "        \n",
    "    value: float, optional\n",
    "        value which is assigned on the grid to the coordinates present in the dataframe\n",
    "        \n",
    "    Returns:\n",
    "    ------\n",
    "    data_array: array\n",
    "        Array with 0 and \"value\" marking the location of the identified events \n",
    "    \n",
    "    \"\"\"\n",
    "    data_array = np.zeros([len(ds[lat_name]),len(ds[lon_name])])\n",
    "    if len(obj) > 0:\n",
    "        for index,row in obj.iterrows():\n",
    "            data_array[int(row[\"lat\"]),int(row[\"lon\"])+int(max(ds[lon_name].values))] = value\n",
    "        return data_array\n",
    "    else:\n",
    "        return data_array\n",
    "\n",
    "# Streamer index    \n",
    "    \n",
    "def get_basepoints(df_contour,geo_dis,cont_dis):\n",
    "    \"\"\"\n",
    "    Extract all basepoint pairs of a given contour. \n",
    "    Basepoints are coordinate pairs that are close regarding their geographical distance (geo_dis), \n",
    "    but far apart regarding their connection on the contour (cont_dis).\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    df_contour: dataframe\n",
    "        Contour in form of a dataframe with the columns lon and lat\n",
    "        \n",
    "    geo_dis: float\n",
    "        Maximal geographical distance between two basepoints in km\n",
    "        \n",
    "    cont_dis: float\n",
    "        Minimal distance between two basepoints on the contour in km\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    list: pair of int\n",
    "        List with pairs of indicies (point position on contour) of all basepoints. \n",
    "        The list does not contain duplicates ((1,5) and (5,1))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Calculate geographical distance between all point pairs\n",
    "    comb = np.asarray(list(itertools.combinations(df_contour.index, r=2)))\n",
    "\n",
    "    df1 = df_contour.iloc[comb[:,0]].reset_index().rename(columns={\"lon\":\"lon1\", \"lat\":\"lat1\", \"index\":\"ind1\"})\n",
    "    df2 = df_contour.iloc[comb[:,1]].reset_index().rename(columns={\"lon\":\"lon2\", \"lat\":\"lat2\", \"index\":\"ind2\"})\n",
    "    df = pd.concat([df1,df2], axis=1)\n",
    "    \n",
    "    km = haversine_np(df['lon1'],df['lat1'],df['lon2'],df['lat2'])\n",
    "    geo_close = df[km<geo_dis][[\"ind1\",\"ind2\"]].values.tolist()\n",
    "    \n",
    "    #Calculate distance along the contour\n",
    "    dis = pd.concat([df_contour,pd.DataFrame(np.roll(df_contour,-1,axis=0), columns=[\"lon_s\",\"lat_s\"])], axis=1)\n",
    "    ongoing_distance = haversine_np(dis['lon'],dis['lat'],dis['lon_s'],dis['lat_s'])\n",
    "\n",
    "    def contour_distance(pair):\n",
    "        \"\"\"\n",
    "        Calculate distance on contour in km using the great circle distance.\n",
    "        \"\"\"\n",
    "        i = min(pair)\n",
    "        j = max(pair)\n",
    "\n",
    "        dis1 = ongoing_distance.values[i:j+1].sum()\n",
    "        dis2 = ongoing_distance.values[j:].sum() + ongoing_distance.values[:i+1].sum()\n",
    "\n",
    "        return min([dis1,dis2])\n",
    "    \n",
    "    #Keep point pairs, which fulfill the requirements\n",
    "    basepoints = [[min(pair),max(pair)] for pair in geo_close if contour_distance(pair) > cont_dis]\n",
    "\n",
    "    return list(map(list,set(map(tuple,basepoints))))\n",
    "\n",
    "def get_streamers(df_contour, bp_list):\n",
    "    \"\"\"\n",
    "    Indentify unique streamers based on the detected basepoints. \n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    df_contour: dataframe\n",
    "        Contour in form of a dataframe with the columns lon and lat\n",
    "    \n",
    "    bp_list: list\n",
    "        List with pairs of indicies (point position on contour) of all basepoints\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    list: dataframe\n",
    "        List of dataframes, each representing a streamer.\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    contour_points = [[df_contour[\"lon\"][i],df_contour[\"lat\"][i]] for i in range(0,len(df_contour))]\n",
    "    cont_line = LineString(contour_points)\n",
    "\n",
    "    def test_intersection(pair):\n",
    "        \"\"\"\n",
    "        Check that the connection between a pair of base points does not intersect with the contour\n",
    "        \"\"\"\n",
    "        i = pair[0]\n",
    "        j = pair[1]\n",
    "\n",
    "        # check 1\n",
    "\n",
    "        point1 = tuple(map(int,(df_contour[\"lat\"][i],df_contour[\"lon\"][i]+180)))\n",
    "        point2 = tuple(map(int,(df_contour[\"lat\"][j],df_contour[\"lon\"][j]+180)))\n",
    "\n",
    "        grid = np.zeros([len(ds[lat_name]),len(ds[lon_name])])\n",
    "        rr,cc = line(point1[0], point1[1], point2[0], point2[1])\n",
    "        grid[rr, cc] = 1\n",
    "        line_points = list(zip(*np.where(grid == 1)))\n",
    "\n",
    "        contour_tuples = [(x[1], x[0]+180) for x in df_contour.to_numpy()]\n",
    "\n",
    "        test = [i for i in line_points if i in contour_tuples and i not in [point1,point2]]\n",
    "\n",
    "        # check 2\n",
    "\n",
    "        lines = LineString([(df_contour[\"lon\"][i],df_contour[\"lat\"][i]),(df_contour[\"lon\"][j],df_contour[\"lat\"][j])])\n",
    "\n",
    "        if test == [] and lines.crosses(cont_line) == False:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    bp_crossing = [pair for pair in bp_list if test_intersection(pair)==False]\n",
    "    \n",
    "    def test_wrapping(pair):\n",
    "        \"\"\"\n",
    "        Drop all pairs that are fully wrapped by another pair\n",
    "        \"\"\"\n",
    "        i = pair[0]\n",
    "        j = pair[1]\n",
    "\n",
    "        a = bp_crossing[i][0] < bp_crossing[j][0]\n",
    "        b = bp_crossing[i][1] > bp_crossing[j][1]\n",
    "\n",
    "        c = bp_crossing[i][0] == bp_crossing[j][0]\n",
    "        d = bp_crossing[i][1] == bp_crossing[j][1]\n",
    "\n",
    "        if (a == True and d == True) or (b == True and c == True):\n",
    "            return j\n",
    "        elif (a == False and d == True) or (b == False and c == True):\n",
    "            return i\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    wrapped = [test_wrapping(pair) for pair in itertools.combinations(range(0,len(bp_crossing)), r=2)]\n",
    "    bp_wrapping = [item for i,item in enumerate(bp_crossing) if i not in wrapped]\n",
    "\n",
    "    def test_overlap(i, j):\n",
    "        \"\"\"\n",
    "        Identify pairs that share over 50% of their connecting contour points and group them together\n",
    "        \"\"\"\n",
    "        i_range = list(range(bp_wrapping[i][0],bp_wrapping[i][1]+1))\n",
    "        j_range = list(range(bp_wrapping[j][0],bp_wrapping[j][1]+1))\n",
    "\n",
    "        overlap = list(set(i_range) & set(j_range))\n",
    "        i_p, j_p = 0,0\n",
    "        if len(i_range) != 0: \n",
    "            i_p = len(overlap)/len(i_range)\n",
    "        if len(j_range) != 0: \n",
    "            j_p = len(overlap)/len(j_range)\n",
    "\n",
    "        if i_p > 0.8 and j_p > 0.8:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    combine = [[i,j] for i,j in itertools.combinations_with_replacement(range(0,len(bp_wrapping)), r=2) if test_overlap(i,j)]\n",
    "\n",
    "    combine_set = []\n",
    "    iterator = combine.copy()\n",
    "    while len(iterator)>0:\n",
    "        first, *rest = iterator\n",
    "        first = set(first)\n",
    "        lf = -1\n",
    "        while len(first)>lf:\n",
    "            lf = len(first)\n",
    "            rest2 = []\n",
    "            for r in rest:\n",
    "                if len(first.intersection(set(r)))>0:\n",
    "                    first |= set(r)\n",
    "                else:\n",
    "                    rest2.append(r)     \n",
    "            rest = rest2\n",
    "        combine_set.append(first)\n",
    "        iterator = rest\n",
    "\n",
    "    def get_unique(group):\n",
    "        \"\"\"\n",
    "        For grouped pairs, keep the one closest to the start and end point of the group\n",
    "        \"\"\"\n",
    "        temp = [bp_wrapping[item] for item in group]\n",
    "        mn = np.asarray(temp)[:,0].min()\n",
    "        mx = np.asarray(temp)[:,1].max()\n",
    "        weight = [sqrt((mn-point[0])**2+(mx-point[1])**2) for point in temp]\n",
    "\n",
    "        return temp[np.argmin(np.asarray(weight))]\n",
    "\n",
    "    bp_unique = [get_unique(group) for group in combine_set]\n",
    "\n",
    "    #return all contour points in a dataframe lying on a connection between two basepoints\n",
    "    return [df_contour[item[0]:item[1]+1] for item in bp_unique]\n",
    "\n",
    "def get_gridpoints_streamer(streamers_list):\n",
    "    \"\"\"\n",
    "    Provide the gridpoints representing the streamer.\n",
    "    The streamer is closed trough a direct connection between start and end point. \n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    streamers_list: list of dataframes\n",
    "        List of dataframes, each representing a streamer in form of contour points \n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    list: dataframes\n",
    "        List of dataframes, each representing all grid points enclosed by a streamer\n",
    "        \n",
    "    \"\"\"\n",
    "    x, y = np.meshgrid(ds[lon_name],ds[lat_name]) \n",
    "    x, y = x.flatten(), y.flatten()\n",
    "    grid_points = np.vstack((x,y)).T  \n",
    "    \n",
    "    def get_grid(streamer):\n",
    "        \"\"\"\n",
    "        Get all grid points enclosed by the streamer path. \n",
    "        \"\"\"\n",
    "        p = Path(np.c_[streamer[\"lon\"], streamer[\"lat\"]])\n",
    "        grid = p.contains_points(grid_points)\n",
    "        \n",
    "        return pd.DataFrame({'lon': grid_points[grid][:,0].tolist(), 'lat': grid_points[grid][:,1].tolist()})\n",
    "    \n",
    "    return [get_grid(streamer) for streamer in streamers_list if len(get_grid(streamer))>0]\n",
    "\n",
    "# Overturning index\n",
    "\n",
    "def get_overturning(df_contour,ot_lons):\n",
    "    \"\"\"\n",
    "    Identification of unique overturning events. \n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    df_contour: dataframe\n",
    "        Contour in form of a dataframe with the columns lon and lat\n",
    "    \n",
    "    ot_lons: list\n",
    "        List longitudes that are crossed at least three times by the contour\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    list: dataframe\n",
    "        List of dataframes, each representing an overturning event.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ot_f0 = [list(df_contour[\"lon\"][df_contour[\"lon\"] == lon].index) for lon in ot_lons]\n",
    "\n",
    "    #Calculate distance along the contour\n",
    "    dis = pd.concat([df_contour,pd.DataFrame(np.roll(df_contour,-1,axis=0), columns=[\"lon_s\",\"lat_s\"])], axis=1)\n",
    "    ongoing_distance = haversine_np(dis['lon'],dis['lat'],dis['lon_s'],dis['lat_s'])\n",
    "\n",
    "    def contour_distance(pair):\n",
    "        \"\"\"\n",
    "        Calculate distance on contour in km using the great circle distance between the members of both pairs.\n",
    "        \"\"\"\n",
    "        i = min(pair)\n",
    "        j = max(pair)\n",
    "\n",
    "        dis1 = ongoing_distance.values[i:j+1].sum()\n",
    "        dis2 = ongoing_distance.values[j:].sum() + ongoing_distance.values[:i+1].sum()\n",
    "\n",
    "        return min([dis1,dis2])\n",
    "\n",
    "    comb = list(itertools.combinations_with_replacement(list(range(0,len(ot_f0))), r=2))\n",
    "\n",
    "    def compare_groups(pair):\n",
    "        \"\"\"\n",
    "        Compare contour distance between groups.\n",
    "        \"\"\"\n",
    "        i = ot_f0[pair[0]]\n",
    "        j = ot_f0[pair[1]]\n",
    "\n",
    "        combs = list(itertools.product(i,j))\n",
    "\n",
    "        for c in combs:\n",
    "            if contour_distance(c) <= cont_apart:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    #Group events that are close enough\n",
    "    comb1 = [pair for pair in comb if compare_groups(pair)]\n",
    "\n",
    "    comb2 = []\n",
    "    iterator = comb1.copy()\n",
    "    while len(iterator)>0:\n",
    "        first, *rest = iterator\n",
    "        first = set(first)\n",
    "        lf = -1\n",
    "        while len(first)>lf:\n",
    "            lf = len(first)\n",
    "            rest2 = []\n",
    "            for r in rest:\n",
    "                if len(first.intersection(set(r)))>0:\n",
    "                    first |= set(r)\n",
    "                else:\n",
    "                    rest2.append(r)     \n",
    "            rest = rest2\n",
    "        comb2.append(first)\n",
    "        iterator = rest\n",
    "\n",
    "    def combine_groups(group):\n",
    "        \"\"\"\n",
    "        Combine groups\n",
    "        \"\"\"\n",
    "        temp1 = [ot_f0[item] for item in group]\n",
    "\n",
    "        temp2 = list(itertools.chain.from_iterable(temp1))\n",
    "\n",
    "        return [min(temp2), max(temp2)]\n",
    "\n",
    "    ot_f1 = [combine_groups(group) for group in comb2]\n",
    "\n",
    "    def test_extent(ls):\n",
    "        \"\"\"\n",
    "        Check that the identified events have a minimal longitudinal extent\n",
    "        \"\"\"\n",
    "        lons = [df_contour[\"lon\"][i] for i in ls]\n",
    "        st = min(lons)\n",
    "        nd = max(lons)\n",
    "        exp = abs(abs(nd)-abs(st))\n",
    "\n",
    "        return exp > lon_exp\n",
    "\n",
    "    ot_f2 = [[min(item),max(item)] for item in ot_f1 if test_extent(item)]\n",
    "    \n",
    "    return [df_contour[item[0]:item[1]+1] for item in ot_f2]\n",
    "\n",
    "def get_gridpoints_ot(ot_list):\n",
    "    \"\"\"\n",
    "    Provide the gridpoints representing the overturning event.\n",
    "    The overturning event is defined by a rectangle overlapping with all grid points contributing to the event.\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    ot_list: list of dataframes\n",
    "        List of dataframes, each representing a overturning event in form of contour points \n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    list: dataframes\n",
    "        List of dataframes, each representing all grid points enclosed by the overturning event\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    lon_range = [int(min(ot_list[\"lon\"])), int(max(ot_list[\"lon\"]))]\n",
    "    lat_range = [int(min(ot_list[\"lat\"])), int(max(ot_list[\"lat\"]))]\n",
    "\n",
    "    if lat_range[0] != lat_range[1]:\n",
    "        lat_range = range(lat_range[0], lat_range[1])\n",
    "    else: \n",
    "        lat_range = lat_range[0]\n",
    "\n",
    "    if lon_range[0] != lon_range[1]:\n",
    "        lon_range = range(lon_range[0], lon_range[1])\n",
    "    else: \n",
    "        lon_range = lon_range[0]\n",
    "    \n",
    "    x, y = np.meshgrid(lon_range,lat_range) \n",
    "    x, y = x.flatten(), y.flatten()\n",
    "    grid_points = np.vstack((x,y)).T \n",
    "    \n",
    "    return pd.DataFrame({'lon': grid_points[:,0].tolist(), 'lat': grid_points[:,1].tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a38b5d-6285-4882-9546-d96b12bbcadd",
   "metadata": {},
   "source": [
    "### Framework Subroutines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f323eb24-bad1-4de8-94a7-4569f2fd87e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index(time):\n",
    "    \"\"\"\n",
    "    Execute the algorithm for one time step and one contour level. \n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    time: datetime64\n",
    "        description of time step for loop application\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    list: arrays\n",
    "        List of two arrays for both distinctions. In each array, 0 means no event detected and 1,2 stands for the two distinctions.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = ds.sel({time_name:time})\n",
    "    \n",
    "    if np.isnan(dataset[var_name]).all() == False:\n",
    "        \n",
    "        \"\"\"\n",
    "        Exectue algorithm for only one level\n",
    "        \"\"\"\n",
    "        #Get contour\n",
    "        contour = extract_contour(dataset, level)\n",
    "        if len(contour) != 0:\n",
    "            \"\"\"\n",
    "            If there is overturning over the date or longitude border, the index calculation tends to deliver wrong results.\n",
    "            Therefore, it is essential to check each contour if there is overturning in this region. \n",
    "            The extent of the region can be changed by the parameter \"lon_window\"\n",
    "            If an overturning is detected, the dataset/contour is rolled by 5 degrees. This is iteratively repeated until the contour passes the check. \n",
    "            After the index calculation, the result is rolled back to the original position. \n",
    "            \"\"\"\n",
    "            #Calculate test quantities\n",
    "            if index == \"streamer\":\n",
    "                basepoints = get_basepoints(contour,geo_dis,cont_dis)\n",
    "                bp_lons = contour[contour.index.isin(set(list(itertools.chain.from_iterable(basepoints))))][\"lon\"].tolist()\n",
    "\n",
    "            if index == \"ot\":\n",
    "                lat_sum = np.sum(map_on_grid(contour), axis=0)\n",
    "                ot_lons = [i-180 for i,x in enumerate(lat_sum) if x >= 3]\n",
    "\n",
    "            roll_point = 0\n",
    "            it = 0\n",
    "            error = 0\n",
    "\n",
    "            while it == 0:\n",
    "                lon_border = [int(ds[lon_name].values.min()), int(ds[lon_name].values.max())]\n",
    "\n",
    "                test_lons = np.asarray(range(lon_border[0]-roll_point-lon_window, lon_border[0]-roll_point+lon_window))\n",
    "                test_lons[test_lons>lon_border[1]]-=360\n",
    "                test_lons[test_lons<lon_border[0]]+=360\n",
    "                test_latsum = sum(np.sum(map_on_grid(contour), axis=0)>1)>len(ds[lon_name].values)/3\n",
    "                if index == \"streamer\":\n",
    "                    #Check if there are undulations over dateborder\n",
    "                    if len(set(bp_lons) & set(test_lons.tolist())) == 0:\n",
    "                        if roll_point != 0:\n",
    "                            dataset = dataset.roll({lon_name:roll_point}, roll_coords=False)\n",
    "                            contour = extract_contour(dataset, level)\n",
    "                            basepoints = get_basepoints(contour,geo_dis,cont_dis)\n",
    "                        it = 1\n",
    "\n",
    "                    else: \n",
    "                        roll_point += 5\n",
    "\n",
    "                if index == \"ot\":\n",
    "                    #Check if there are undulations over dateborder\n",
    "                    if len(set(ot_lons) & set(test_lons.tolist())) == 0:\n",
    "                        if roll_point != 0:\n",
    "                            dataset = dataset.roll({lon_name:roll_point}, roll_coords=False)\n",
    "                            contour = extract_contour(dataset, level)\n",
    "                            lat_sum = np.sum(map_on_grid(contour), axis=0)\n",
    "                            ot_lons = [i-180 for i,x in enumerate(lat_sum) if x >= 3]\n",
    "                        it = 1\n",
    "\n",
    "                    else: \n",
    "                        roll_point += 5\n",
    "\n",
    "                if roll_point > 360 or test_latsum == True:\n",
    "                    error = 1\n",
    "                    it = 1\n",
    "            #Continue, if there is a valid contour\n",
    "            if error == 0:\n",
    "                if index == \"streamer\":\n",
    "                    streamers = get_streamers(contour, basepoints) \n",
    "                    streamers_grid = get_gridpoints_streamer(streamers)\n",
    "                    #Distinguish between stratospheric and tropospheric\n",
    "                    def stratos(grids_list):\n",
    "                        return [np.average(np.asarray([dataset[var_name][int(grid[\"lat\"][i]),int(grid[\"lon\"][i]-lon_border[0])].values for i in range(0,len(grid))])) > level for grid in grids_list]\n",
    "\n",
    "                    test_stratos = stratos(streamers_grid)\n",
    "                    stratos = list(itertools.compress(streamers_grid, test_stratos))\n",
    "                    tropos = list(itertools.compress(streamers_grid,[not item for item in test_stratos]))\n",
    "\n",
    "                    if stratos != []:\n",
    "                        stratos = pd.concat(stratos)\n",
    "                    if tropos != []:\n",
    "                        tropos = pd.concat(tropos)\n",
    "\n",
    "                    #Distinguish between cyclonic and anticyclonic by kinematic momentum flux\n",
    "                    if \"flux\" in list(ds.keys()):\n",
    "\n",
    "                        def lc12(grids_list):\n",
    "                            return [np.average(np.asarray([dataset[\"flux\"][int(grid[\"lat\"][i]),int(grid[\"lon\"][i]-lon_border[0])].values for i in range(0,len(grid))])) < 0 for grid in grids_list]\n",
    "\n",
    "                        test_lc12 = lc12(streamers_grid)\n",
    "                        cc = list(itertools.compress(streamers_grid,test_lc12))\n",
    "                        ac = list(itertools.compress(streamers_grid,[not item for item in test_lc12]))\n",
    "\n",
    "                        if cc != []:\n",
    "                            cc = pd.concat(cc)\n",
    "                        if ac != []:\n",
    "                            ac = pd.concat(ac)\n",
    "                    else:\n",
    "                        cc = []\n",
    "                        ac = []\n",
    "\n",
    "                    #Roll back the results if \"roll_point\" was not zero and map it on a grid\n",
    "                    if roll_point != 0:\n",
    "                        res_stratos = np.roll(map_on_grid(stratos), -roll_point, axis=1)\n",
    "                        res_tropos = np.roll(map_on_grid(tropos, value = 2), -roll_point, axis=1)\n",
    "                        res_cc = np.roll(map_on_grid(cc), -roll_point, axis=1)\n",
    "                        res_ac = np.roll(map_on_grid(ac, value = 2), -roll_point, axis=1)\n",
    "                    else:\n",
    "                        res_stratos = map_on_grid(stratos)\n",
    "                        res_tropos = map_on_grid(tropos, value = 2)\n",
    "                        res_cc = map_on_grid(cc)\n",
    "                        res_ac = map_on_grid(ac, value = 2)\n",
    "\n",
    "                    #Add result togther and set every values larger than 2 to 1\n",
    "                    res1 = res_stratos+res_tropos\n",
    "                    res2 = res_cc + res_ac\n",
    "                    res1[res1>2] = 1\n",
    "                    res2[res2>2] = 1\n",
    "\n",
    "\n",
    "                if index==\"ot\":\n",
    "                    overturnings = get_overturning(contour, ot_lons)\n",
    "                    ot_grid = [get_gridpoints_ot(item) for item in overturnings]\n",
    "\n",
    "                    #Distinguish between cyclonic and anticyclonic by orientation of the events\n",
    "                    def orientation(events):\n",
    "                        return [item[lat_name].values[0] <= item[lat_name].values[-1] for item in events]\n",
    "\n",
    "                    test_orientation = orientation(overturnings)\n",
    "                    occ = list(itertools.compress(ot_grid,test_orientation))\n",
    "                    oac = list(itertools.compress(ot_grid,[not item for item in test_orientation]))\n",
    "\n",
    "                    if occ != []:\n",
    "                        occ = pd.concat(occ)\n",
    "                    if oac != []:\n",
    "                        oac = pd.concat(oac)\n",
    "\n",
    "                    #Distinguish between cyclonic and anticyclonic by kinematic momentum flux\n",
    "                    if \"flux\" in list(ds.keys()):\n",
    "\n",
    "                        def lc12(grids_list):\n",
    "                            return [np.average(np.asarray([dataset[\"flux\"][int(grid[\"lat\"][i]),int(grid[\"lon\"][i]-lon_border[0])].values for i in range(0,len(grid))])) < 0 for grid in grids_list]\n",
    "\n",
    "                        test_lc12 = lc12(ot_grid)\n",
    "                        cc = list(itertools.compress(ot_grid,test_lc12))\n",
    "                        ac = list(itertools.compress(ot_grid,[not item for item in test_lc12]))\n",
    "\n",
    "                        if cc != []:\n",
    "                            cc = pd.concat(cc)\n",
    "                        if ac != []:\n",
    "                            ac = pd.concat(ac)\n",
    "\n",
    "                    else:\n",
    "                        cc = []\n",
    "                        ac = []\n",
    "\n",
    "                    #Roll back the results if \"roll_point\" was not zero and map it on a grid\n",
    "                    if roll_point != 0:\n",
    "                        res_occ = np.roll(map_on_grid(occ), -roll_point, axis=1)\n",
    "                        res_oac = np.roll(map_on_grid(oac, value = 2), -roll_point, axis=1)\n",
    "                        res_cc = np.roll(map_on_grid(cc), -roll_point, axis=1)\n",
    "                        res_ac = np.roll(map_on_grid(ac, value = 2), -roll_point, axis=1)\n",
    "                    else:\n",
    "                        res_occ = map_on_grid(occ)\n",
    "                        res_oac = map_on_grid(oac, value = 2)\n",
    "                        res_cc = map_on_grid(cc)\n",
    "                        res_ac = map_on_grid(ac, value = 2)\n",
    "\n",
    "                    #Add result togther and set every values larger than 2 to 1\n",
    "                    res1 = res_occ + res_oac\n",
    "                    res2 = res_cc + res_ac\n",
    "                    res1[res1>2] = 1\n",
    "                    res2[res2>2] = 1\n",
    "\n",
    "            else:\n",
    "                res1 = np.zeros([len(ds[lat_name]),len(ds[lon_name])])\n",
    "                res2 = np.zeros([len(ds[lat_name]),len(ds[lon_name])])\n",
    "\n",
    "        else:\n",
    "            res1 = np.zeros([len(ds[lat_name]),len(ds[lon_name])])\n",
    "            res2 = np.zeros([len(ds[lat_name]),len(ds[lon_name])])\n",
    "\n",
    "    else:\n",
    "        res1 = np.zeros([len(ds[lat_name]),len(ds[lon_name])])\n",
    "        res2 = np.zeros([len(ds[lat_name]),len(ds[lon_name])])\n",
    "    \n",
    "    return [res1, res2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edd7f6-5103-423a-8ae2-deaa7f92a34f",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc711eff-ae30-4841-9b8c-6450d87b2400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enter parameters for framework test\n",
    "# the parameters defines the size of the window in degrees for a possible start point search\n",
    "# in this window, no undulations should be present\n",
    "lon_window = 10\n",
    "\n",
    "# enter parameters for streamer detection algorithm\n",
    "# define the (max) geographical and the (min) distance on the contour between two possible base points, defining a streamer\n",
    "geo_dis = 800\n",
    "cont_dis = 1500\n",
    "\n",
    "# enter parameters for overturning detection algorithm\n",
    "# cont_apart: events closer than cont_apart are added together\n",
    "# lon_exp: the detected event need the be larger than a specific longitudinal extension in degrees\n",
    "cont_apart = 500\n",
    "lon_exp = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f49736-205f-47bf-829e-3c614b72fe89",
   "metadata": {},
   "source": [
    "### run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aad10b-b7f2-41c3-9880-ecb641982b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 342/365 [13:38<00:58,  2.55s/it]"
     ]
    }
   ],
   "source": [
    "# run algorithm. A progress bar is included\n",
    "result = [get_index(time) for i,time in zip(tqdm(range(len(ds[time_name]))), ds[time_name])]\n",
    "\n",
    "# in case of the index \"streamer\": \n",
    "    # flag_1: 0 = no streamer present; 1 = stratospheric streamer; 2 = tropospheric streamer\n",
    "    # flag_2: 0 = no streamer present; 1 = cyclonic streamer; 2 = anticyclonic streamer\n",
    "\n",
    "# in case of the index \"ot\": \n",
    "    # flag_1: 0 = no streamer present; 1 = cyclonic ot; 2 = anticyclonic ot; DISTINCTION BY ORIENTATION\n",
    "    # flag_2: 0 = no streamer present; 1 = cyclonic ot; 2 = anticyclonic ot; DISTINCTION BY KINEMATIC FLUX\n",
    "\n",
    "ds[\"flag_1\"] = ((time_name,lat_name, lon_name), [item[0] for item in result])\n",
    "ds[\"flag_2\"] = ((time_name,lat_name, lon_name),  [item[1] for item in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf36840-57ba-44b9-9934-e84201a99e2c",
   "metadata": {},
   "source": [
    "## Visualilzation\n",
    "### Plot climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0a043-64a7-4ced-8b64-073011e6a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new cmap\n",
    "import matplotlib.colors as colors\n",
    "def truncate_colormap(cmap, minval=0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "cmap = plt.get_cmap(\"RdYlBu_r\")\n",
    "new_cmap = truncate_colormap(cmap, 0.2, 1)\n",
    "new_cmap.set_bad(color=\"whitesmoke\")\n",
    "\n",
    "data_crs = ccrs.PlateCarree()\n",
    "proj = ccrs.NorthPolarStereo()\n",
    "\n",
    "fig, axes = plt.subplots(2,2, subplot_kw=dict(projection=proj), figsize=(14,14))\n",
    "\n",
    "plt.subplots_adjust(left  = 0.125,  # the left side of the subplots of the figure\n",
    "                    right = 0.9,    # the right side of the subplots of the figure\n",
    "                    bottom = 0.1,   # the bottom of the subplots of the figure\n",
    "                    top = 0.9,      # the top of the subplots of the figure\n",
    "                    wspace = 0.4,   # the amount of width reserved for blank space between subplots\n",
    "                    hspace = -0.4   # the amount of height reserved for white space between subplots)\n",
    "                   )\n",
    "\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "center, radius = [0.5, 0.5], 0.5\n",
    "verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n",
    "circle = mpath.Path(verts * radius + center)\n",
    "\n",
    "lev = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "d1 = xr.where(ds.flag_1==1,1,0).sum(dim=\"time\")/len(ds.time)*100\n",
    "p1 = d1.where(d1>0).plot(ax=axes[0,0], cmap = new_cmap, transform=data_crs, levels = lev, add_colorbar=False, extend = \"max\")\n",
    "\n",
    "d2 = xr.where(ds.flag_1==2,1,0).sum(dim=\"time\")/len(ds.time)*100\n",
    "p2 = d2.where(d2>0).plot(ax=axes[0,1], cmap = new_cmap, transform=data_crs, levels = lev, add_colorbar=False, extend = \"max\")\n",
    "\n",
    "d3 = xr.where(ds.flag_2==1,1,0).sum(dim=\"time\")/len(ds.time)*100\n",
    "p3 = d3.where(d3>0).plot(ax=axes[1,0], cmap = new_cmap, transform=data_crs, levels = lev, add_colorbar=False, extend = \"max\")\n",
    "\n",
    "d4 = xr.where(ds.flag_2==2,1,0).sum(dim=\"time\")/len(ds.time)*100\n",
    "p4 = d4.where(d4>0).plot(ax=axes[1,1], cmap = new_cmap, transform=data_crs, levels = lev, add_colorbar=False, extend = \"max\")\n",
    "  \n",
    "cbar = plt.colorbar(p1, ax=axes.flat, shrink=0.7, drawedges=True, pad = 0.1)\n",
    "cbar.ax.set_yticklabels(lev, fontsize=22, weight='bold')\n",
    "cbar.set_label(label=\"Occurrence frequency in %\",size=22, fontweight=\"bold\",labelpad=15)\n",
    "\n",
    "cbar.outline.set_color('black')\n",
    "cbar.outline.set_linewidth(2)\n",
    "\n",
    "cbar.dividers.set_color('black')\n",
    "cbar.dividers.set_linewidth(2)\n",
    "\n",
    "if index ==\"ot\":\n",
    "    titles = [\"Cyclonic overturning by orientation\", \"Anticyclonic overturning by orientation\", \"Cyclonic overturning by momentum flux\", \"Anticyclonic overturning by momentum flux\"]\n",
    "if index ==\"streamer\":\n",
    "    titles = [\"Stratospheric streamers\", \"Tropospheric streamers\", \"Cyclonic streamers\", \"Anticyclonic streamers\"]\n",
    "    \n",
    "for ax,title in zip(axes.flat, titles):\n",
    "    ax.set_extent([-180, 180, 10, 89], crs=data_crs)\n",
    "    ax.add_feature(cfeature.COASTLINE, color=\"dimgrey\")\n",
    "    gr = ax.gridlines(draw_labels=True, color=\"black\", linestyle=\"dotted\", linewidth = 1.1)\n",
    "    gr.xlabel_style = {'size': 16, 'color': 'black', \"rotation\":0}\n",
    "    gr.ylabel_style = {'size': 14, 'color': 'black'}\n",
    "    ax.set_boundary(circle, transform=ax.transAxes)\n",
    "    ax.add_patch(mpatches.Circle((0.5, 0.5), radius=0.5, color='k', linewidth=4, fill=False, transform = ax.transAxes))\n",
    "    ax.set_title(\"\")\n",
    "    ax.set_title(title, fontweight='bold',fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_data)",
   "language": "python",
   "name": "env_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
